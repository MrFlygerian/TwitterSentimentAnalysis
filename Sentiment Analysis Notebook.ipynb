{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.99 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bless\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bless\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import twitter\n",
    "import csv\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation \n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob \n",
    "import numpy as np\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "twitter_api = twitter.Api(consumer_key ='8i68MvUnFgp3Mqh5jRV34jGh3',\n",
    "                        consumer_secret= 'jNPsW7ZKZkKoVLAvFjwgTPHvlw4Hto8jbpnCDcbnnSUm7O8RJX',\n",
    "                        access_token_key='2378518193-5lkOIoa6OJpm3X0AehYz9EAjws6NjSsjHZS93MM',\n",
    "                        access_token_secret = 'DN6j3LxuKWtkKJkGpU16pUr2wqSFYoUs6FwED1SXFjpAw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building the test data set\n",
    "def buildTestSet(search_keyword):\n",
    "    try:\n",
    "        tweets_fetched = twitter_api.GetSearch(search_keyword, count = 100, lang = \"en\")\n",
    "        \n",
    "        print(\"Fetched \" + str(len(tweets_fetched)) + \" tweets for the term \" + search_keyword)\n",
    "        \n",
    "        return [{\"text\":status.text, \"label\":None} for status in tweets_fetched]\n",
    "    except:\n",
    "        return None\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is done in a seperate file'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#Building the training data set\n",
    "\"\"\"This is done in a seperate file\"\"\"\n",
    "#import TrainingTweetsBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AccessCSV(train_data):\n",
    "    trainDataSet = [] \n",
    "    with open(train_data,'r', encoding = \"utf-8\") as csvfile:\n",
    "        lineReader = csv.reader(csvfile,delimiter=',', quotechar=\"\\\"\")\n",
    "        for row in lineReader:\n",
    "                trainDataSet.append(row)\n",
    "    return trainDataSet\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create and implement PreProcessor\n",
    "class PreProcessTweets:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._stopwords = set(stopwords.words('english') + list(punctuation) + ['AT_USER','URL'])\n",
    "        \n",
    "    def processTweets(self, list_of_tweets):\n",
    "        processedTweets=[]\n",
    "        for tweet in list_of_tweets:\n",
    "            if any(isinstance(tweet,dict) for tweet in list_of_tweets):\n",
    "                try:\n",
    "                    processedTweets.append((self._processTweet(tweet[\"text\"]),tweet[\"label\"]))\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "            else:\n",
    "                try:\n",
    "                    processedTweets.append((self._processTweet(tweet[1]),tweet[2]))\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                \n",
    "        return processedTweets\n",
    "    \n",
    "    def _processTweet(self, tweet):\n",
    "        tweet = tweet.lower() # convert text to lower-case\n",
    "        tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', 'URL', tweet) # remove URLs\n",
    "        tweet = re.sub('@[^\\s]+', 'AT_USER', tweet) # remove usernames\n",
    "        tweet =  re.sub(r'#([^\\s]+)', r'\\1', tweet) # remove the # in #hashtag\n",
    "        tweet =  word_tokenize(tweet) # remove repeated characters (helloooooooo into hello)\n",
    "        \n",
    "        return [word for word in tweet if word not in self._stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepping the model\n",
    "def buildVocabulary(data_to_train):\n",
    "    all_words = []\n",
    "    \n",
    "    for (words, sentiment) in data_to_train:\n",
    "        all_words.extend(words)\n",
    "\n",
    "    wordlist = nltk.FreqDist(all_words)\n",
    "    word_features = wordlist.keys()\n",
    "    return word_features\n",
    "def extract_features(tweet):\n",
    "    tweet_words = set(tweet)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word] = (word in tweet_words)\n",
    "    return features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The whole program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search on Twitter:George Floyd\n",
      "Fetched 100 tweets for the term George Floyd\n",
      "[{'text': 'For George Floyd.\\nFor Breonna Taylor.\\nFor our children.\\nFor \\u200ball\\u200b of us.\\n\\nAnd for the lives of too many others to n… https://t.co/76y6vV6ADw', 'label': None}, {'text': 'Omg Kamala just said with a straight face “The Coronavirus is racist and knows how we see and treat each other” and… https://t.co/ACnKmxiGPV', 'label': None}, {'text': 'The freak who brutally attacked an innocent passerby in Portland is not just an activist — he’s a self-identified B… https://t.co/cITjdC52wl', 'label': None}, {'text': 'Office Williams Justice for George Floyd https://t.co/WBkBnqUu7u via @YouTube', 'label': None}, {'text': 'RT @bluelivesmtr: Attorney Files To Dismiss Charges In George Floyd Case Saying He Died Of Overdose - The Police Tribune https://t.co/0tqi5…', 'label': None}]\n",
      "Wall time: 5.79 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Build test set\n",
    "search_term = input(\"Search on Twitter:\")\n",
    "testDataSet = buildTestSet(search_term)\n",
    "print(testDataSet[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 20 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Access saved training set\n",
    "train_data =  \"tweetDataFile.csv\"     \n",
    "trainDataSet = list(filter(None, AccessCSV(train_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Clean the training and test data, prepare features (words in the tweets) and target (sentiment)\n",
    "tweetProcessor = PreProcessTweets()\n",
    "data_to_train = tweetProcessor.processTweets(trainDataSet)\n",
    "data_to_test = tweetProcessor.processTweets(testDataSet)\n",
    "word_features = buildVocabulary(data_to_train)\n",
    "trainingFeatures = nltk.classify.apply_features(extract_features, data_to_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Train the learning model (in this case, a Naive Bayes Classifier) and apply testing tweets to test tweets \n",
    "NBayesClassifier = nltk.NaiveBayesClassifier.train(trainingFeatures)\n",
    "NBResultLabels = [NBayesClassifier.classify(extract_features(tweet[0])) for tweet in data_to_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 323 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X = 0\n",
    "Y = 0 \n",
    "Z = 0 \n",
    "testDataSet_copy = testDataSet.copy()\n",
    "for tweet in testDataSet_copy:\n",
    "    if  TextBlob(tweet['text']).sentiment.polarity > 0:\n",
    "        tweet['label'] = \"positive\"\n",
    "        X+= 1\n",
    "    elif TextBlob(tweet['text']).sentiment.polarity < 0:\n",
    "        tweet['label'] = \"negative\"\n",
    "        Y += 1\n",
    "    else:\n",
    "        tweet['label'] = \"neutral\"\n",
    "        Z += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = []\n",
    "for tweet in testDataSet_copy:\n",
    "    sentiment.append(TextBlob(tweet['text']).sentiment.polarity)\n",
    "real_sentiment = np.mean(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = 0\n",
    "total = len(testDataSet)\n",
    "for i, label in enumerate(NBResultLabels):\n",
    "    if label == testDataSet_copy[i]['label']:\n",
    "        matches += 1\n",
    "accuracy = 100*(matches/total)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: Overall Negative Sentiment\n",
      "Negative Sentiment Percentage = 3.0%\n",
      "  \n",
      "Actaul: Overall Positive sentiment\n",
      "Positive Sentiment Percentage = 42.0%\n",
      "  \n",
      "The sentiment of 46 out of 100 tweets was predicted correctly\n",
      "Accuracy of prediction = 46.0%\n"
     ]
    }
   ],
   "source": [
    "if NBResultLabels.count('positive') > NBResultLabels.count('negative'):\n",
    "    print(\"Predicted: Overall Positive Sentiment\")\n",
    "    print(f\"Positive Sentiment Percentage = {100*NBResultLabels.count('positive')/len(NBResultLabels)}%\")\n",
    "else: \n",
    "    print(\"Predicted: Overall Negative Sentiment\")\n",
    "    print(f\"Negative Sentiment Percentage = {100*NBResultLabels.count('negative')/len(NBResultLabels)}%\")\n",
    "\n",
    "print (\"  \")\n",
    "\n",
    "#get the real sentiment\n",
    "if real_sentiment > 0:\n",
    "    print ('Actaul: Overall Positive sentiment')\n",
    "    print(f\"Positive Sentiment Percentage = {100*X/total}%\")\n",
    "    \n",
    "elif real_sentiment < 0:\n",
    "    print (\"Actual: Overall Negative Sentiment\")\n",
    "    print(f\"Negative Sentiment Percentage = {100*Y/total}%\")\n",
    "\n",
    "else:\n",
    "    print (f\"Actual: Overal Neutral Sentment with a percentage of {100*Z/total}%'\")\n",
    "    \n",
    "print(\"  \")\n",
    "print(f\"The sentiment of {matches} out of {total} tweets was predicted correctly\")\n",
    "print(f'Accuracy of prediction = {round(accuracy,2)}%')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
